// src/main/java/progscala2/bigdata/HadoopWordCount.javaX
package bigdata;

import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import java.util.StringTokenizer;

public class HadoopWordCount {

  public static void main(String[] args) {
    String input  = args[0];
    String output = args[1];

    JobClient client = new JobClient();
    JobConf conf = new JobConf(WordCount.class);

    conf.setJobName("WordCount");
    conf.setOutputKeyClass(Text.class);
    conf.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(conf, new Path(input));
    FileOutputFormat.setOutputPath(conf, new Path(output));
    conf.setMapperClass(WordCountMapper.class);
    conf.setReducerClass(WordCountReducer.class);

    client.setConf(conf);

    try {
      JobClient.runJob(conf);
    } catch (Exception e) {
      e.printStackTrace();
    }
  }

  class WordCountMapper extends MapReduceBase
      implements Mapper<IntWritable, Text, Text, IntWritable> {

    static final IntWritable one  = new IntWritable(1);
    // Value will be set in a non-thread-safe way!
    static final Text word = new Text;

    @Override
    public void map(IntWritable key, Text valueDocContents,
        OutputCollector<Text, IntWritable> output, Reporter reporter) {
    String[] tokens = valueDocContents.toString.split("\\s+");       // <2>
      for (String wordString: tokens) {
        if (wordString.length > 0) {
          word.set(wordString.toLowerCase);
          output.collect(word, one);
        }
      }
    }
  }

  class WordCountReduce extends MapReduceBase
    implements Reducer<Text, IntWritable, Text, IntWritable> {

    public void reduce(Text keyWord, java.util.Iterator<IntWritable> counts,
      OutputCollector<Text, IntWritable> output, Reporter reporter) {
    int totalCount = 0;
    while (counts.hasNext) {                                         // <3>
      while (counts.hasNext) {
        totalCount += counts.next.get;
      }
      output.collect(keyWord, new IntWritable(totalCount));
    }
  }
}